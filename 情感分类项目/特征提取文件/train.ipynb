{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e37574e-382d-473f-ba43-71bacb36b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Sequence, WhitespaceSplit, Punctuation, Digits\n",
    "import re\n",
    "import jieba\n",
    "import pickle\n",
    "import torch.utils.checkpoint as checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c391cc-cd86-4d87-9d18-02654d1d7d00",
   "metadata": {},
   "source": [
    "训练tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "166c5d9a-e481-446a-9bb3-208a6c7bcb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(xml_text):\n",
    "    \"\"\"从XML格式文本中加载句子和标签\"\"\"\n",
    "    import re\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    # 使用正则表达式提取每个review\n",
    "    pattern = r'<review id=\"\\d+\"\\s+label=\"(\\d)\">(.*?)</review>'\n",
    "    matches = re.findall(pattern, xml_text, re.DOTALL)\n",
    "    \n",
    "    for label, text in matches:\n",
    "        sentences.append(text.strip())\n",
    "        labels.append(int(label))\n",
    "    \n",
    "    return sentences, labels\n",
    "def load_data_from_file(file_path):\n",
    "    \"\"\"从XML文件加载数据\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf8',errors='ignore') as f:\n",
    "        xml_text = f.read()\n",
    "    return load_data(xml_text)\n",
    "sentences_cn,labels_cn=load_data_from_file(\"Sentiment Classification with Deep Learning/test.label.cn.txt\")\n",
    "sentences_en,labels_en=load_data_from_file(\"Sentiment Classification with Deep Learning/test.label.en.txt\")\n",
    "sentences=sentences_cn+sentences_en\n",
    "labels=labels_cn+labels_en\n",
    "sentences_ids=[]\n",
    "for i in sentences:\n",
    "    sentences_ids.append(tokenizer.encode(i, add_special_tokens=False).ids)\n",
    "def test(model,sentences_ids,labels,loss_F,device=\"cuda\"):\n",
    "    labels_tensor=[]\n",
    "    for i in labels:\n",
    "        if i==1:\n",
    "            labels_tensor.append(torch.tensor([[1.,0.]],device=device))\n",
    "        else:\n",
    "            labels_tensor.append(torch.tensor([[0.,1.]],device=device))\n",
    "    total_loss=0\n",
    "    for i in range(len(sentences_ids)):\n",
    "        model.eval()\n",
    "        model.to(device=device)\n",
    "        _,out=model.forward(torch.tensor([sentences_ids[i]]).to(device=device))\n",
    "        total_loss+=(loss_F(out,labels_tensor[i]).to(device=device)).mean().cpu()/len(sentences_ids)\n",
    "    return float(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa44f2b2-53fe-42ff-853d-88e791777473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def load_reviews(file_path):\n",
    "    \"\"\"加载评论数据并提取句子\"\"\"\n",
    "    reviews = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # 使用正则表达式提取所有<review>标签内的内容\n",
    "    pattern = r'<review[^>]*>(.*?)</review>'\n",
    "    matches = re.findall(pattern, content, re.DOTALL)\n",
    "    \n",
    "    # 清理提取的文本（去除首尾空白，合并换行）\n",
    "    for match in matches:\n",
    "        # 去除空白字符，并合并多行\n",
    "        cleaned_text = match.strip()\n",
    "        reviews.append(cleaned_text)\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e65d2a-e55d-4ad0-b5c6-d580c2ae9487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "14987\n",
      "19985\n"
     ]
    }
   ],
   "source": [
    "data1=load_reviews(\"./evaltask2_sample_data/cn_sample_data/sample.positive.txt\")\n",
    "data_t=list(torch.tensor([[1,0]]).repeat(len(data1),1))\n",
    "print(len(data1))\n",
    "data1=data1+load_reviews(\"./evaltask2_sample_data/cn_sample_data/sample.negative.txt\")\n",
    "data_t=data_t+list(torch.tensor([[0,1]]).repeat(len(data1)-len(data_t),1))\n",
    "print(len(data1))\n",
    "data1=data1+load_reviews(\"./evaltask2_sample_data/en_sample_data/sample.positive.txt\")\n",
    "data_t=data_t+list(torch.tensor([[1,0]]).repeat(len(data1)-len(data_t),1))\n",
    "print(len(data1))\n",
    "data1=data1+load_reviews(\"./evaltask2_sample_data/en_sample_data/sample.negative.txt\")\n",
    "data_t=data_t+list(torch.tensor([[0,1]]).repeat(len(data1)-len(data_t),1))\n",
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e139432-82f0-4cf5-bb23-b987848ae459",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"cn_Whitespace.txt\",\"a\",encoding=\"utf8\")\n",
    "for i in data1:\n",
    "    seg_list = jieba.lcut(i)\n",
    "    f.write(\" \".join(seg_list)+\"\\n\")\n",
    "f.close()\n",
    "\n",
    "f=open(\"en_Whitespace.txt\",\"a\",encoding=\"utf8\")\n",
    "for i in data2:\n",
    "    f.write(i.replace(\"\\n\", \"\")+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2103fb40-63bf-43ce-aab9-4988dd024446",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))  # 使用BPE算法\n",
    "tokenizer.pre_tokenizer = Sequence([\n",
    "    WhitespaceSplit(),\n",
    "    Punctuation(behavior=\"isolated\"),\n",
    "    Digits(individual_digits=False),\n",
    "])\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=30000,  # 目标词表大小\n",
    "    special_tokens=[\"[UNK]\",\"[PAD]\"], # 特殊标记\n",
    "    min_frequency=10  # 最小出现频次\n",
    ")\n",
    "\n",
    "files = [\"cn_Whitespace.txt\",\"en_Whitespace.txt\"]  # 你的训练文本路径\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "tokenizer.save(\"./my_tokenizer.json\")  # 保存\n",
    "# 加载\n",
    "tokenizer = Tokenizer.from_file(\"./my_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d104-577d-4555-ba06-1f0021d1c976",
   "metadata": {},
   "source": [
    "构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1f74582-f5e7-4a4d-8986-0e1c5aa66543",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"./my_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb652c5-7bcc-4728-9c54-53dbd4d45880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def bucket_and_pad_with_dynamic_buckets(texts0, tokenizer, pad_id=1, min_bucket=8):\n",
    "    \"\"\"\n",
    "    动态计算桶大小并分桶填充\n",
    "    \"\"\"\n",
    "    data_t=texts0[1]\n",
    "    texts=texts0[0]\n",
    "    \n",
    "    # 计算最大长度\n",
    "    max_len = max(len(tokenizer.encode(text, add_special_tokens=False)) for text in tqdm(texts))\n",
    "    \n",
    "    # 生成桶大小（2的幂次方，直到大于最大长度）\n",
    "    bucket_sizes = []\n",
    "    size = min_bucket\n",
    "    while size < max_len:\n",
    "        bucket_sizes.append(size)\n",
    "        size *= 2\n",
    "    bucket_sizes.append(max(size, max_len))\n",
    "    \n",
    "    # 分桶填充\n",
    "    buckets = {size: [] for size in bucket_sizes}\n",
    "    \n",
    "    for idx,text in tqdm(enumerate(texts)):\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False).ids\n",
    "        token_len = len(tokens)\n",
    "        \n",
    "        # 选择桶大小\n",
    "        for size in bucket_sizes:\n",
    "            if token_len <= size:\n",
    "                padded = tokens + [pad_id] * (size - token_len)\n",
    "                buckets[size].append([padded,data_t[idx]])\n",
    "                break\n",
    "    \n",
    "    return buckets, bucket_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec59341-cb97-48d2-a4e8-3fb64a266751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 19985/19985 [00:02<00:00, 8520.97it/s]\n",
      "19985it [00:02, 8369.46it/s] \n"
     ]
    }
   ],
   "source": [
    "bucket,bucket_sizes=bucket_and_pad_with_dynamic_buckets((data1,data_t),tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bb5209b-4ad7-4b4b-b5d5-5546d3318bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['请问', '这机', '不是', '有个', '遥', '控器', '的', '吗', '？']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('请问这机不是有个遥控器的吗？', add_special_tokens=False).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e3aa39-9a5b-45bb-973c-c048040c9510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a0a9b-bc95-408c-b84d-c4e9049eaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###删除过长序列\n",
    "bucket.pop(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948b84ff-4110-4743-bf7b-dee76b6774b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_batches_from_buckets_with_shuffle(buckets, batch_size=64, pad_token_id=1, shuffle=True):\n",
    "    all_batches = []\n",
    "    \n",
    "    for bucket_size, texts_in_bucket in buckets.items():\n",
    "        if not texts_in_bucket:\n",
    "            continue\n",
    "        \n",
    "        # 如果需要，打乱当前桶内的文本顺序\n",
    "        if shuffle:\n",
    "            random.shuffle(texts_in_bucket)\n",
    "        \n",
    "        # 按batch_size分组\n",
    "        for i in range(0, len(texts_in_bucket), batch_size):\n",
    "            batch_texts = texts_in_bucket[i:i+batch_size]\n",
    "            # 创建attention mask\n",
    "            attention_masks = [\n",
    "                [1 if token != pad_token_id else 0 for token in text_tokens[0]]\n",
    "                for text_tokens in batch_texts\n",
    "            ]\n",
    "            \n",
    "            all_batches.append((batch_texts, attention_masks))\n",
    "    \n",
    "    # 如果需要，打乱所有batch的顺序\n",
    "    if shuffle:\n",
    "        random.shuffle(all_batches)\n",
    "    \n",
    "    return all_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8be12b7-0ada-40df-993f-4301e6c64649",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = create_batches_from_buckets_with_shuffle(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42828a55-668a-4389-bb21-4414741bce74",
   "metadata": {},
   "source": [
    "保存构建好的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb41c123-67dc-4318-9166-eff50ce24dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('file.pkl', 'wb') as f:\n",
    "    pickle.dump(all_batches, f)  # 序列化到文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55b798-79c3-4112-b08f-0fdf6be4734a",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f79faa0-fa22-4be0-961f-615994e37752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('file.pkl', 'rb') as f:\n",
    "    all_batches = pickle.load(f)  # 从文件反序列化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f279c5a1-489a-47e0-865e-41067320d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "class DINOImageDataset(Dataset):\n",
    "    \"\"\"DINO训练的自定义数据集\"\"\"\n",
    "    def __init__(self, all_batches, transform=None):\n",
    "        super(DINOImageDataset, self).__init__()\n",
    "        self.data = all_batches\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data[idx]\n",
    "train_loader=DINOImageDataset(all_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16dcd3db-41f9-4a9b-b9ec-9d175b2ae732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=96, num_layers=2, \n",
    "                 output_size=256, dropout=0., vocab_size=30000, embedding_dim=128):\n",
    "        super(ImprovedLSTMModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx=1)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # 添加嵌入层的dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 添加更多全连接层\n",
    "        self.fc1 = nn.Linear(hidden_size*4, 512)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        self.fc2 = nn.Linear(512, output_size)\n",
    "        \n",
    "        self.fc3 = nn.Linear(output_size, 512)\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        self.fc4 = nn.Linear(512, 2)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        \n",
    "        lstm_out, (hidden, cell) = checkpoint.checkpoint(self.lstm,x,use_reentrant=False)\n",
    "        hidden_final = torch.cat((hidden[-2, :, :], hidden[-1, :, :],F.adaptive_max_pool1d(lstm_out.permute(0, 2, 1), output_size=1).squeeze(2)), dim=1)\n",
    "        # 更多非线性变换\n",
    "        out = self.fc1(hidden_final)\n",
    "        out = self.silu(out)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        out0 = self.fc3(out)\n",
    "        out0 = self.silu(out0)\n",
    "        out0 = self.layer_norm(out0)\n",
    "        out0 = self.fc4(out0)\n",
    "        out0 = self.softmax(out0)\n",
    "        return F.normalize(out),out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034df1da-2dd2-4835-9d50-8f047a3e0c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============== 1. 定义核心组件 ==============\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Sequence, WhitespaceSplit, Punctuation, Digits\n",
    "import re\n",
    "import jieba\n",
    "import pickle\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "class MCRLoss(nn.Module):\n",
    "    def __init__(self, ncrops, reduce_cov=0, expa_type=0, eps=0.1, coeff=0.2):\n",
    "        super().__init__()\n",
    "        self.ncrops = ncrops\n",
    "        self.eps = eps\n",
    "        self.coeff = coeff\n",
    "        self.reduce_cov = reduce_cov\n",
    "        self.expa_type = expa_type\n",
    "\n",
    "    def forward(self, student_feat, teacher_feat):\n",
    "        \"\"\"\n",
    "        Expansion Loss and Compression Loss between features of the teacher and student networks.\n",
    "        \"\"\"\n",
    "        student_feat = student_feat.view(self.ncrops, -1, student_feat.shape[-1])\n",
    "        teacher_feat = teacher_feat.view(2, -1, teacher_feat.shape[-1])\n",
    "        \n",
    "        comp_loss = self.calc_compression(student_feat, teacher_feat)\n",
    "        if self.expa_type == 0: # only compute expansion on global views\n",
    "            expa_loss = self.calc_expansion(student_feat[:len(teacher_feat)])\n",
    "        elif self.expa_type == 1:\n",
    "            expa_loss = self.calc_expansion((student_feat[:len(teacher_feat)]+teacher_feat)/2)\n",
    "        loss = - self.coeff * comp_loss - expa_loss\n",
    "        return loss, comp_loss.detach(), expa_loss.detach()\n",
    "    \n",
    "    def calc_compression(self, student_feat_list, teacher_feat_list):\n",
    "        \"\"\"\n",
    "        Compute compression loss between student and teacher features.\n",
    "        \"\"\"\n",
    "        # Convert lists of tensors to a single tensor for vectorized operations\n",
    "        \n",
    "        sim = F.cosine_similarity(teacher_feat_list.unsqueeze(1), student_feat_list.unsqueeze(0), dim=-1)\n",
    "        sim.view(-1, sim.shape[-1])[:: (len(student_feat_list) + 1), :].fill_(0)  # Trick to fill diagonal\n",
    "        \n",
    "        n_loss_terms = len(teacher_feat_list)* len(student_feat_list) - min(len(teacher_feat_list), len(student_feat_list))\n",
    "        # Sum the cosine similarities\n",
    "        comp_loss = sim.mean(2).sum()/n_loss_terms\n",
    "        # global_comp_loss = (sim[:, :len(teacher_feat_list)].mean(2).sum()).detach_().div_(len(teacher_feat_list))\n",
    "        return comp_loss\n",
    "    \n",
    "    def calc_expansion(self, feat_list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute expansion loss using Coding Rate estimation.\n",
    "        \"\"\"\n",
    "        cov_list = []\n",
    "        num_views = len(feat_list)\n",
    "        m, p = feat_list[0].shape\n",
    "        \n",
    "        cov_list = [W.T.matmul(W) for W in feat_list]\n",
    "        cov_list = torch.stack(cov_list)\n",
    "        N=1\n",
    "\n",
    "        scalar = p / (m * N * self.eps)\n",
    "        I = torch.eye(p, device=cov_list[0].device)\n",
    "        loss:torch.Tensor = 0\n",
    "        for i in range(num_views):\n",
    "            loss += torch.linalg.cholesky_ex(I + scalar * cov_list[i])[0].diagonal().log().sum()\n",
    "        loss /= num_views\n",
    "        loss *= (p+N*m)/(p*N*m) # the balancing factor gamma, you can also use the next line. This is ultimately a heuristic, so feel free to experiment.\n",
    "        # loss *= ((self.eps * N * m) ** 0.5 / p)\n",
    "        return loss\n",
    "def monitor_features(feats):\n",
    "    # 特征范数（应该接近1，因为L2归一化）\n",
    "    norms = torch.norm(feats, dim=1).mean().item()\n",
    "    \n",
    "    # 特征间的平均余弦相似度（应该适中，不是0也不是1）\n",
    "    sim_matrix = feats @ feats.T\n",
    "    avg_sim = (sim_matrix.sum() - feats.shape[0]) / (feats.shape[0] * (feats.shape[0]-1))\n",
    "    \n",
    "    # 特征协方差矩阵的秩（应该接近特征维度）\n",
    "    cov = feats.T @ feats / feats.shape[0]\n",
    "    rank = torch.linalg.matrix_rank(cov).item()\n",
    "    \n",
    "    return norms, avg_sim, rank\n",
    "# ============== 2. 数据增强 ==============\n",
    "\n",
    "def generate_sorted_random(k, min_val, max_val, device='cpu'):\n",
    "    n = max_val - min_val\n",
    "    if k > n:\n",
    "        raise ValueError(f\"k={k}不能大于范围大小{n}\")\n",
    "    indices = torch.randperm(n, device=device)[:k]\n",
    "    sorted_indices = torch.sort(indices).values + min_val\n",
    "    return sorted_indices\n",
    "def remove_elements(tensor, indices_to_remove):\n",
    "    mask = torch.ones(len(tensor), dtype=torch.bool)\n",
    "    mask[indices_to_remove] = False\n",
    "    return tensor[mask]\n",
    "#def random_remove_elements_batch_vectorized(tensor, mask, remove_ratio=0.2, seed=None, device='cuda'):\n",
    "def random_remove_elements_batch_vectorized(tensor, mask, remove_ratio=0.2, seed=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    更快的批量随机移除元素版本，使用完全的向量化操作\n",
    "    \"\"\"\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        tensor = tensor.to(device)\n",
    "        mask = mask.to(device)\n",
    "    \n",
    "    batch_size, seq_len = tensor.shape[:2]\n",
    "    remove_num = int(seq_len * remove_ratio)\n",
    "    \n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "    mask=mask>0\n",
    "    # 生成随机矩阵用于选择要移除的位置\n",
    "    random_matrix = torch.rand(batch_size, seq_len, device=device)\n",
    "    \n",
    "    # 将无效位置（mask=False）的随机值设为很大，确保不会被选为最小\n",
    "    random_matrix = random_matrix.masked_fill(~mask, float('inf'))\n",
    "    \n",
    "    # 对于每个样本，选择remove_num个随机值最小的位置\n",
    "    _, remove_indices = torch.topk(random_matrix, remove_num, dim=1, largest=False)\n",
    "    \n",
    "    # 创建移除掩码\n",
    "    remove_mask = torch.zeros((batch_size, seq_len), dtype=torch.bool, device=device)\n",
    "    \n",
    "    # 使用scatter_快速设置移除位置\n",
    "    batch_indices = torch.arange(batch_size, device=device).unsqueeze(1).expand(-1, remove_num)\n",
    "    remove_mask.scatter_(1, remove_indices, True)\n",
    "    \n",
    "    # 保留掩码 = 有效掩码 AND 非移除掩码\n",
    "    keep_mask = mask & (~remove_mask)\n",
    "    \n",
    "    # 重新组织张量\n",
    "    # 计算每个样本保留的元素数量\n",
    "    keep_counts = keep_mask.sum(dim=1)\n",
    "    max_keep = keep_counts.max().item()\n",
    "    \n",
    "    # 创建结果张量\n",
    "    if len(tensor.shape) == 2:\n",
    "        result = torch.ones((batch_size, max_keep), dtype=tensor.dtype, device=device)\n",
    "    else:\n",
    "        result = torch.ones((batch_size, max_keep, *tensor.shape[2:]), \n",
    "                           dtype=tensor.dtype, device=device)\n",
    "    \n",
    "    # 填充结果\n",
    "    for i in range(batch_size):\n",
    "        keep_indices = torch.nonzero(keep_mask[i], as_tuple=True)[0]\n",
    "        result[i, :len(keep_indices)] = tensor[i][keep_indices]\n",
    "    \n",
    "    return result\n",
    "def build_cropped_text(text, mask, crop_rate, device='cuda'):\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        text = text.cuda()\n",
    "        mask = mask.cuda()\n",
    "    return [random_remove_elements_batch_vectorized(text, mask, rate, device=device) for rate in crop_rate]\n",
    "\n",
    "\n",
    "class twin_model:\n",
    "    def __init__(self, student, teacher, device):\n",
    "        self.student = student.to(device)\n",
    "        self.teacher = teacher.to(device)\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.loss_F=MCRLoss(9)\n",
    "\n",
    "    def update_teacher(self, momentum):\n",
    "        \"\"\"教师模型EMA更新\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 获取学生和教师模型的参数\n",
    "            student_params = dict(self.student.named_parameters())\n",
    "            teacher_params = dict(self.teacher.named_parameters())\n",
    "            \n",
    "            # 应用EMA更新：teacher = momentum * teacher + (1 - momentum) * student\n",
    "            for name in teacher_params:\n",
    "                teacher_params[name].data.mul_(momentum).add_(\n",
    "                    student_params[name].data, alpha=1 - momentum\n",
    "                )\n",
    "    def student_forward(self,x):\n",
    "        return self.student(x)\n",
    "    def teacher_forward(self,x):\n",
    "        return self.teacher(x)\n",
    "    def cal_loss(self,student,teacher):\n",
    "        return self.loss_F.forward(student,teacher)\n",
    "def load_data(xml_text):\n",
    "    \"\"\"从XML格式文本中加载句子和标签\"\"\"\n",
    "    import re\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    # 使用正则表达式提取每个review\n",
    "    pattern = r'<review id=\"\\d+\"\\s+label=\"(\\d)\">(.*?)</review>'\n",
    "    matches = re.findall(pattern, xml_text, re.DOTALL)\n",
    "    \n",
    "    for label, text in matches:\n",
    "        sentences.append(text.strip())\n",
    "        labels.append(int(label))\n",
    "    \n",
    "    return sentences, labels\n",
    "def load_data_from_file(file_path):\n",
    "    \"\"\"从XML文件加载数据\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf8',errors='ignore') as f:\n",
    "        xml_text = f.read()\n",
    "    return load_data(xml_text)\n",
    "\n",
    "\n",
    "def test(model,sentences_ids,labels,loss_F,device=\"cuda\"):\n",
    "    labels_tensor=[]\n",
    "    for i in labels:\n",
    "        if i==1:\n",
    "            labels_tensor.append(torch.tensor([[1.,0.]],device=device))\n",
    "        else:\n",
    "            labels_tensor.append(torch.tensor([[0.,1.]],device=device))\n",
    "    total_loss=0\n",
    "    model.eval()\n",
    "    model.to(device=device)\n",
    "    for i in range(len(sentences_ids)):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            _,out=model.forward(torch.tensor([sentences_ids[i]]).to(device=device))\n",
    "            total_loss+=(loss_F(out,labels_tensor[i]).to(device=device)).mean().cpu()/len(sentences_ids)\n",
    "    return float(total_loss)\n",
    "\n",
    "def train_dino_simple(\n",
    "    twin_model,\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.,\n",
    "    momentum_schedule=None,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    简化版DINO训练\n",
    "    \"\"\"\n",
    "    device=device\n",
    "    # 初始化训练器\n",
    "    \n",
    "    # 初始化优化器\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        twin_model.student.parameters(),\n",
    "        lr=lr,\n",
    "    )\n",
    "    tokenizer = Tokenizer.from_file(\"./my_tokenizer.json\")\n",
    "    target_loss_F=nn.CrossEntropyLoss()\n",
    "    # 训练循环\n",
    "    sentences_cn,labels_cn=load_data_from_file(\"Sentiment Classification with Deep Learning/test.label.cn.txt\")\n",
    "    sentences_en,labels_en=load_data_from_file(\"Sentiment Classification with Deep Learning/test.label.en.txt\")\n",
    "    sentences=sentences_cn+sentences_en\n",
    "    labels=labels_cn+labels_en\n",
    "    sentences_ids=[]\n",
    "    for i in sentences:\n",
    "        sentences_ids.append(tokenizer.encode(i, add_special_tokens=False).ids)\n",
    "    print(\"test_loss : \"+str(test(twin_model.student,sentences_ids,labels,target_loss_F)))\n",
    "    for epoch in range(1,epochs+1):\n",
    "        twin_model.student.train()\n",
    "        total_loss = 0\n",
    "        total_comp_loss =0\n",
    "        total_expa_loss =0\n",
    "        if momentum_schedule is not None:\n",
    "            momentum = momentum_schedule[epoch]\n",
    "        else:\n",
    "            momentum = 0.996\n",
    "        T=tqdm(train_loader,total = len(train_loader))\n",
    "        e_loss=[]\n",
    "        e_tar_loss=[]\n",
    "        e_norms=[]\n",
    "        e_avg_sim=[]\n",
    "        e_rank=[]\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch=list(batch)\n",
    "            batch[0],target=(zip(*batch[0]))\n",
    "            batch=torch.tensor(batch)\n",
    "            T.set_description(f'Epoch [{batch_idx+1}/{len(train_loader)}]')\n",
    "            # 训练步骤\n",
    "            global_input=[batch[0]]+build_cropped_text(batch[0],batch[1],[0.05])\n",
    "            local_input=build_cropped_text(batch[0],batch[1],[0.1,0.15,0.2,0.25,0.3,0.35,0.4])\n",
    "            \n",
    "            teacher_input=global_input\n",
    "            student_input=global_input+local_input\n",
    "            \n",
    "            teacher_outputs = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in range(len(teacher_input)):\n",
    "                    teacher_out,_ = twin_model.teacher_forward(teacher_input[i].to(device))\n",
    "                    teacher_outputs.append(F.normalize(teacher_out, p=2, dim=-1))\n",
    "                teacher_outputs = torch.cat(teacher_outputs, dim=0)\n",
    "            # ============ 学生模型前向传播 ============\n",
    "            # 学生模型处理所有裁剪\n",
    "            tar_loss=0\n",
    "            student_outputs = []\n",
    "            for i in range(len(student_input)):\n",
    "                student_out,cla_out = twin_model.student_forward(student_input[i].to(device))\n",
    "                tar_loss+=target_loss_F(cla_out,torch.stack(target).to(device=device,dtype=torch.float))*(1.5 + (0.2 - 1.5) * (i / len(student_input)))/len(student_input)\n",
    "                student_outputs.append(F.normalize(student_out, p=2, dim=-1))\n",
    "            student_outputs = torch.cat(student_outputs, dim=0)  # [(2+n_local)*batch_size, out_dim]\n",
    "            # ============ 计算损失 ============\n",
    "            # 重复教师目标以匹配学生输出的数量\n",
    "            \n",
    "            loss,comp_loss,expa_loss=twin_model.cal_loss(student_outputs,teacher_outputs)\n",
    "            loss=loss*np.sin((epoch / epochs)*np.pi/2)**2+tar_loss*np.cos((epoch / epochs)*np.pi/2)**2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            test_tensor=student_outputs.clone().detach()\n",
    "            norms, avg_sim, rank = monitor_features(test_tensor.cpu())\n",
    "            \n",
    "            twin_model.update_teacher(momentum)\n",
    "            total_loss += loss\n",
    "            total_comp_loss += comp_loss\n",
    "            total_expa_loss += expa_loss\n",
    "            e_tar_loss.append(float(tar_loss.detach()))\n",
    "            e_loss.append(float(loss.detach()))\n",
    "            e_norms  .append(norms)\n",
    "            e_avg_sim.append(avg_sim)\n",
    "            e_rank   .append(rank)\n",
    "            \n",
    "            T.set_postfix(loss=np.mean(e_loss[(batch_idx-50) if batch_idx>50 else 0:batch_idx]),\n",
    "                          norms=np.mean(e_norms  [(batch_idx-50) if batch_idx>50 else 0:batch_idx]),\n",
    "                          avg_sim=np.mean(e_avg_sim[(batch_idx-50) if batch_idx>50 else 0:batch_idx]),\n",
    "                          rank=np.mean(e_rank   [(batch_idx-50) if batch_idx>50 else 0:batch_idx]),\n",
    "                          tar_loss=np.mean(e_tar_loss   [(batch_idx-50) if batch_idx>50 else 0:batch_idx]),\n",
    "                         )\n",
    "            T.update(1)\n",
    "        T.close()\n",
    "        # 打印epoch统计\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_comp_loss = total_comp_loss / len(train_loader)\n",
    "        avg_expa_loss = total_expa_loss / len(train_loader)\n",
    "        \n",
    "\n",
    "        print(f\"Epoch {epoch} completed. Avg Loss: {avg_loss:.4f}, Avg comp Loss: {avg_comp_loss:.4f}, Avg expa Loss: {avg_expa_loss:.4f}, Momentum: {momentum:.4f}\")\n",
    "        if (epoch) % 5 == 0:\n",
    "            print(\"test_loss : \"+str(test(twin_model.student,sentences_ids,labels,target_loss_F)))\n",
    "        # 保存检查点\n",
    "        if (epoch) % 10 == 0:\n",
    "            torch.save({\n",
    "                'student_state_dict': twin_model.student.state_dict(),\n",
    "                'teacher_state_dict': twin_model.teacher.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'loss': avg_loss,\n",
    "            }, f'checkpoint_epoch_{epoch}.pth')\n",
    "    \n",
    "    return twin_model.student, twin_model.teacher\n",
    "\n",
    "\n",
    "\n",
    "student_model = ImprovedLSTMModel(num_layers=2)\n",
    "teacher_model = ImprovedLSTMModel(num_layers=2)\n",
    "\n",
    "\n",
    "#teacher_model.load_state_dict(student_model.state_dict())\n",
    "twin_model=twin_model(student_model,teacher_model,\"cuda\")\n",
    "\n",
    "\n",
    "device=\"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fb7be-b849-44f7-9df5-df479c45ccf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss : 0.7025463581085205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/318]:   0%|                                                                           | 0/318 [00:00<?, ?it/s]E:\\dinov3_test\\venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "E:\\dinov3_test\\venv\\lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch [318/318]: 100%|█| 318/318 [01:00<00:00,  5.27it/s, avg_sim=0.647, loss=0.583, norms=1, rank=150, tar_loss=0.584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 0.6104, Avg comp Loss: 0.3384, Avg expa Loss: 0.6227, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|██| 318/318 [01:02<00:00,  5.12it/s, avg_sim=0.363, loss=0.525, norms=1, rank=185, tar_loss=0.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Avg Loss: 0.5502, Avg comp Loss: 0.7191, Avg expa Loss: 0.8909, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.15it/s, avg_sim=0.209, loss=0.485, norms=1, rank=223, tar_loss=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Avg Loss: 0.5080, Avg comp Loss: 0.7709, Avg expa Loss: 1.2114, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.19it/s, avg_sim=0.125, loss=0.451, norms=1, rank=237, tar_loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Avg Loss: 0.4817, Avg comp Loss: 0.7257, Avg expa Loss: 1.4734, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|██| 318/318 [01:01<00:00,  5.17it/s, avg_sim=0.069, loss=0.404, norms=1, rank=248, tar_loss=0.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Avg Loss: 0.4343, Avg comp Loss: 0.6975, Avg expa Loss: 1.7593, Momentum: 0.9960\n",
      "test_loss : 0.5496216416358948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.21it/s, avg_sim=0.0559, loss=0.379, norms=1, rank=250, tar_loss=0.446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed. Avg Loss: 0.3962, Avg comp Loss: 0.6891, Avg expa Loss: 1.8865, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.15it/s, avg_sim=0.0561, loss=0.338, norms=1, rank=250, tar_loss=0.431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed. Avg Loss: 0.3575, Avg comp Loss: 0.6820, Avg expa Loss: 1.9529, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.17it/s, avg_sim=0.042, loss=0.293, norms=1, rank=251, tar_loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Avg Loss: 0.3225, Avg comp Loss: 0.6751, Avg expa Loss: 2.0029, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.21it/s, avg_sim=0.0324, loss=0.247, norms=1, rank=251, tar_loss=0.402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed. Avg Loss: 0.2689, Avg comp Loss: 0.6751, Avg expa Loss: 2.0715, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.16it/s, avg_sim=0.0416, loss=0.207, norms=1, rank=251, tar_loss=0.398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed. Avg Loss: 0.2189, Avg comp Loss: 0.6675, Avg expa Loss: 2.1047, Momentum: 0.9960\n",
      "test_loss : 0.5365180373191833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.19it/s, avg_sim=0.0219, loss=0.147, norms=1, rank=251, tar_loss=0.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 completed. Avg Loss: 0.1707, Avg comp Loss: 0.6610, Avg expa Loss: 2.1278, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:00<00:00,  5.22it/s, avg_sim=0.0298, loss=0.108, norms=1, rank=251, tar_loss=0.381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 completed. Avg Loss: 0.1188, Avg comp Loss: 0.6554, Avg expa Loss: 2.1527, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:00<00:00,  5.24it/s, avg_sim=0.027, loss=0.0662, norms=1, rank=251, tar_loss=0.386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 completed. Avg Loss: 0.0759, Avg comp Loss: 0.6460, Avg expa Loss: 2.1585, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:01<00:00,  5.18it/s, avg_sim=0.0212, loss=0.00114, norms=1, rank=251, tar_loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 completed. Avg Loss: 0.0156, Avg comp Loss: 0.6462, Avg expa Loss: 2.1807, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:00<00:00,  5.22it/s, avg_sim=0.0216, loss=-0.053, norms=1, rank=251, tar_loss=0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 completed. Avg Loss: -0.0397, Avg comp Loss: 0.6388, Avg expa Loss: 2.1867, Momentum: 0.9960\n",
      "test_loss : 0.5598496794700623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [318/318]: 100%|█| 318/318 [01:00<00:00,  5.23it/s, avg_sim=0.0273, loss=-0.0972, norms=1, rank=251, tar_loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 completed. Avg Loss: -0.0915, Avg comp Loss: 0.6355, Avg expa Loss: 2.1963, Momentum: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [254/318]:  80%|▊| 253/318 [00:51<00:14,  4.35it/s, avg_sim=0.0194, loss=-0.169, norms=1, rank=254, tar_loss=0.35"
     ]
    }
   ],
   "source": [
    "trained_student, trained_teacher = train_dino_simple(\n",
    "    twin_model,\n",
    "    train_loader,\n",
    "    epochs=200,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7413bb5-bf19-4576-9366-ea3590a2a973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImprovedLSTMModel(\n",
       "  (embedding): Embedding(30000, 128, padding_idx=1)\n",
       "  (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lstm): LSTM(128, 96, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=384, out_features=512, bias=True)\n",
       "  (silu): SiLU()\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=twin_model.student\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0fd501-22fd-4d96-a72b-897f1cf6479c",
   "metadata": {},
   "source": [
    "测试代码，把文件都丢到目录下就可以运行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc9c634-6b75-46b4-8031-527400994949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "import re\n",
    "import pickle\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "class ImprovedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=96, num_layers=2, \n",
    "                 output_size=256, dropout=0., vocab_size=30000, embedding_dim=128):\n",
    "        super(ImprovedLSTMModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx=1)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # 添加嵌入层的dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 添加更多全连接层\n",
    "        self.fc1 = nn.Linear(hidden_size*4, 512)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        self.fc2 = nn.Linear(512, output_size)\n",
    "        \n",
    "        self.fc3 = nn.Linear(output_size, 512)\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        self.fc4 = nn.Linear(512, 2)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        \n",
    "        lstm_out, (hidden, cell) = checkpoint.checkpoint(self.lstm,x,use_reentrant=False)\n",
    "        hidden_final = torch.cat((hidden[-2, :, :], hidden[-1, :, :],F.adaptive_max_pool1d(lstm_out.permute(0, 2, 1), output_size=1).squeeze(2)), dim=1)\n",
    "        # 更多非线性变换\n",
    "        out = self.fc1(hidden_final)\n",
    "        out = self.silu(out)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        out0 = self.fc3(out)\n",
    "        out0 = self.silu(out0)\n",
    "        out0 = self.layer_norm(out0)\n",
    "        out0 = self.fc4(out0)\n",
    "        out0 = self.softmax(out0)\n",
    "        return F.normalize(out),out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f33dbd-7d5a-43a8-a906-b863d77348b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9987/9987 [00:10<00:00, 929.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 9998/9998 [00:09<00:00, 1024.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def load_checkpoint(model,path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['student_state_dict'])\n",
    "    return model\n",
    "def load_reviews(file_path):\n",
    "    reviews = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    pattern = r'<review[^>]*>(.*?)</review>'\n",
    "    matches = re.findall(pattern, content, re.DOTALL)\n",
    "    for match in matches:\n",
    "        cleaned_text = match.strip()\n",
    "        reviews.append(cleaned_text)\n",
    "    return reviews\n",
    "net = ImprovedLSTMModel(num_layers=2)\n",
    "net=load_checkpoint(net,\"checkpoint_epoch_200 - 副本.pth\").to(\"cuda\")\n",
    "net.eval()\n",
    "data_p=load_reviews(\"./evaltask2_sample_data/cn_sample_data/sample.positive.txt\")\n",
    "data_n=load_reviews(\"./evaltask2_sample_data/cn_sample_data/sample.negative.txt\")\n",
    "data_p=data_p+load_reviews(\"./evaltask2_sample_data/en_sample_data/sample.positive.txt\")\n",
    "data_n=data_n+load_reviews(\"./evaltask2_sample_data/en_sample_data/sample.negative.txt\")\n",
    "tokenizer = Tokenizer.from_file(\"./my_tokenizer.json\")\n",
    "feature_p=None\n",
    "for i in tqdm(data_p):\n",
    "    if feature_p==None:\n",
    "        with torch.no_grad():\n",
    "            feature_p,_=net(torch.tensor([tokenizer.encode(i, add_special_tokens=False).ids]).to(\"cuda\"))\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            feature_p=torch.cat([feature_p,net(torch.tensor([tokenizer.encode(i, add_special_tokens=False).ids]).to(\"cuda\"))[0]])\n",
    "feature_n=None\n",
    "for i in tqdm(data_n):\n",
    "    if feature_n==None:\n",
    "        with torch.no_grad():\n",
    "            feature_n,_=net(torch.tensor([tokenizer.encode(i, add_special_tokens=False).ids]).to(\"cuda\"))\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            feature_n=torch.cat([feature_n,net(torch.tensor([tokenizer.encode(i, add_special_tokens=False).ids]).to(\"cuda\"))[0]])\n",
    "feature_p=feature_p.cpu().numpy()\n",
    "feature_n=feature_n.cpu().numpy()\n",
    "with open('feature_p.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_p, f)  # 序列化到文件\n",
    "with open('feature_n.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_n, f)  # 序列化到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c467addc-c7fb-4dd7-8ed1-e4b2fbb8c15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:04<00:00, 1136.71it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(model,path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['student_state_dict'])\n",
    "    return model\n",
    "def load_data(xml_text):\n",
    "    \"\"\"从XML格式文本中加载句子和标签\"\"\"\n",
    "    import re\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    pattern = r'<review id=\"\\d+\"\\s+label=\"(\\d)\">(.*?)</review>'\n",
    "    matches = re.findall(pattern, xml_text, re.DOTALL)\n",
    "    for label, text in matches:\n",
    "        sentences.append(text.strip())\n",
    "        labels.append(int(label))\n",
    "    return sentences, labels\n",
    "def load_data_from_file(file_path):\n",
    "    \"\"\"从XML文件加载数据\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf8',errors='ignore') as f:\n",
    "        xml_text = f.read()\n",
    "    return load_data(xml_text)\n",
    "net = ImprovedLSTMModel(num_layers=2)\n",
    "net=load_checkpoint(net,\"checkpoint_epoch_200 - 副本.pth\").to(\"cuda\")\n",
    "net.eval()\n",
    "sentences_cn,labels_cn=load_data_from_file(\"Sentiment Classification with Deep Learning/test.label.cn.txt\")\n",
    "sentences_en,labels_en=load_data_from_file(\"Sentiment Classification with Deep Learning/test.label.en.txt\")\n",
    "sentences=sentences_cn+sentences_en\n",
    "labels=labels_cn+labels_en\n",
    "sentences_ids=[]\n",
    "for i in sentences:\n",
    "    sentences_ids.append(tokenizer.encode(i, add_special_tokens=False).ids)\n",
    "sentences_feature=[]\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(sentences_ids):\n",
    "        feature,_=net(torch.tensor([i]).to(\"cuda\"))\n",
    "        sentences_feature.append(feature[0].cpu().numpy())\n",
    "    with open('sentences_feature.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_feature, f)  # 序列化到文件\n",
    "    with open('labels.pkl', 'wb') as f:\n",
    "        pickle.dump(labels, f)  # 序列化到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e01ff4be-3be5-4944-8cd9-6068ddc7b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "def load_data(xml_text):\n",
    "    import re\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    pattern = r'<review id=\"\\d+\"\\s+label=\"(\\d)\">(.*?)</review>'\n",
    "    matches = re.findall(pattern, xml_text, re.DOTALL)\n",
    "    for label, text in matches:\n",
    "        sentences.append(text.strip())\n",
    "        labels.append(int(label))\n",
    "    return sentences, labels\n",
    "def load_data_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf8',errors='ignore') as f:\n",
    "        xml_text = f.read()\n",
    "    return load_data(xml_text)\n",
    "def test(model,sentences_ids,labels,device=\"cuda\"):\n",
    "    labels_tensor=[]\n",
    "    pred=[]\n",
    "    for i in labels:\n",
    "        if i==1:\n",
    "            labels_tensor.append(0)\n",
    "        else:\n",
    "            labels_tensor.append(1)\n",
    "    total_loss=0\n",
    "    model.eval()\n",
    "    model.to(device=device)\n",
    "    for i in range(len(sentences_ids)):\n",
    "        with torch.no_grad():\n",
    "            _,out=model.forward(torch.tensor([sentences_ids[i]]).to(device=device))\n",
    "            pred.append(torch.argmax(out.cpu()))\n",
    "    print(f\"准确率 (Accuracy): {accuracy_score(labels_tensor, pred):.4f}\")\n",
    "    print(\"\\n分类详情:\")\n",
    "    print(classification_report(labels_tensor, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5faba1cd-7737-4d78-9131-fa551fc8fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.7688\n",
      "\n",
      "分类详情:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.77      2500\n",
      "           1       0.76      0.78      0.77      2500\n",
      "\n",
      "    accuracy                           0.77      5000\n",
      "   macro avg       0.77      0.77      0.77      5000\n",
      "weighted avg       0.77      0.77      0.77      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_cn,labels_cn=load_data_from_file(\"Sentiment Classification with Deep Learning/test.label.cn.txt\")\n",
    "sentences_en,labels_en=load_data_from_file(\"Sentiment Classification with Deep Learning/test.label.en.txt\")\n",
    "sentences=sentences_cn+sentences_en\n",
    "labels=labels_cn+labels_en\n",
    "sentences_ids=[]\n",
    "for i in sentences:\n",
    "    sentences_ids.append(tokenizer.encode(i, add_special_tokens=False).ids)\n",
    "test(net,sentences_ids,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3927d-cd84-4fca-b566-6a5d8e2b9224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
